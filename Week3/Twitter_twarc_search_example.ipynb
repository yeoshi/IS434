{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe54afca",
   "metadata": {},
   "source": [
    "In this lab, we will use 'twarc' Python package to interact with Twitter API v2.\n",
    "\n",
    "We will use Search Tweets --> Recent search (endpoint)\n",
    "\n",
    "This API endpoint:\n",
    "- (By default) Retrieves 10 tweets in the recent 7 days\n",
    "- (By setting max_results to 100) Can retrieve up to 100 tweets per API request\n",
    "\n",
    "Ref: https://twarc-project.readthedocs.io/en/latest/api/client2/#twarc.client2.Twarc2.search_recent\n",
    "\n",
    "Ref: https://www.youtube.com/watch?v=guHH51GDDI0\n",
    "\n",
    "Ref: https://catriscode.com/2021/05/01/tweets-cleaning-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from twarc import Twarc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72de0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste in your own bearer token below\n",
    "client = Twarc2(\n",
    "    bearer_token = ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'covid lang=en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.search_recent(\n",
    "    query=query,\n",
    "    max_results=100,\n",
    "    tweet_fields=\"author_id,context_annotations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in results:\n",
    "    print(\"======================\")\n",
    "    print(page)\n",
    "    data = page['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bae225",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1280c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON into Pandas data frame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's retrieve just the text from each tweet\n",
    "# Store each tweet text as a List item\n",
    "\n",
    "tweet_text_list = df['text'].tolist()\n",
    "\n",
    "tweet_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Lowercasing all the letters\n",
    "\n",
    "This step is important to make sure that all your letters are in uniform.\n",
    "\n",
    "temp = tweet.lower()\n",
    "temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2) Removing hashtags and mentions\n",
    "\n",
    "Hashtags and mentions are common in tweets.\n",
    "There are cases where you want to remove them so you only get the \n",
    "clean content of a tweet without all these elements.\n",
    "\n",
    "You can remove these hashtags and mentions using regex.\n",
    "\n",
    "import re\n",
    "\n",
    "temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71867b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3) Removing links\n",
    "\n",
    "Links are usually not necessary for text processing, so it’s\n",
    "better to remove them from your text.\n",
    "\n",
    "temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "temp = re.sub(r\"www.\\S+\", \"\", temp)\n",
    "temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4639ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4) Removing punctuations\n",
    "\n",
    "Depending on your needs, you may not need punctuations such as\n",
    "period, comma, exclamation mark, question mark, etc.\n",
    "\n",
    "temp = re.sub('[()!?]', ' ', temp)\n",
    "temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5) Filtering non-alphanumeric characters\n",
    "\n",
    "The previous step may have removed the punctuations, including all the\n",
    "non-alphanumeric characters, but just to be sure, we can remove all letters\n",
    "except the alphabets (a-z) and numbers (0-9). The sign ^ below means except.\n",
    "\n",
    "temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb751be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6) Tokenization\n",
    "\n",
    "In tokenization, you basically tokenize your text into tokens.\n",
    "\n",
    "And what is a token? In this case, you split your text into smaller components,\n",
    "for example a paragraph into a list of sentences, or a sentence into a list of words.\n",
    "\n",
    "Library such as nltk provides functions such as word_tokenize() or sent_tokenize()\n",
    "to help you with this.\n",
    "\n",
    "However, if you just want a simple tokenizing step where you split your text \n",
    "into words into a list, then you can do it as simple as the following code.\n",
    "\n",
    "The result will give you a list of words from your text.\n",
    "\n",
    "temp = temp.split()\n",
    "temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7) Stop words removal\n",
    "\n",
    "Stop words are words that are considered unimportant to the meaning of a text.\n",
    "These words may seem important to us, humans, but to machine these words may be\n",
    "considered nuisance to the processing steps.\n",
    "\n",
    "It’s also important to keep in mind that stop words are largely language-dependent.\n",
    "In English, you have stop words such as for, to, and, or, in, out, etc.\n",
    "\n",
    "Here I first defined a list of stop words in English.\n",
    "Then, I match each token with each stop word.\n",
    "\n",
    "If a token isn’t found in the list of stop words, the token gets saved,\n",
    "otherwise it’s not saved. In the end, you join all the words into one text again.\n",
    "\n",
    "\n",
    "stopwords = [\"for\", \"on\", \"an\", \"a\", \"of\", \"and\", \"in\", \"the\", \"to\", \"from\"]\n",
    " \n",
    "temp = [w for w in temp if not w in stopwords]\n",
    "temp = \" \".join(word for word in temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ae91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Text Preprocessing: From Start to Finish\n",
    "\n",
    "I hope you understand the steps I have explained above.\n",
    "Now we can combine all those lines of code into one function that we can\n",
    "call and pass an argument to.\n",
    "\n",
    "The function then returns a clean text that is ready for you to work with.\n",
    "\n",
    "Keep in mind that the order of steps here are not absolute.\n",
    "You can arrange them around depending on your text and your needs.\n",
    "The code below is what I found to be the most effective on the data I\n",
    "usually work with, but in case you find another pattern of data,\n",
    "you can always work them out differently.\n",
    "'''\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "stopwords = [\"for\", \"on\", \"an\", \"a\", \"of\", \"and\", \"in\", \"the\", \"to\", \"from\"]\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    if type(tweet) == np.float:\n",
    "        return \"\"\n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stopwords]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\"Get ready for #NatGeoEarthDay! Join us on 4/21 for an evening of music and celebration, exploration and inspiration https://on.natgeo.com/3t0wzQy.\",\n",
    "\"Coral in the shallows of Aitutaki Lagoon, Cook Islands, Polynesia https://on.natgeo.com/3gkgq4Z\",\n",
    "\"Don't miss our @reddit AMA with author and climber Mark Synnott who will be answering your questions about his historic journey to the North Face of Everest TODAY at 12:00pm ET! Start submitting your questions here: https://on.natgeo.com/3ddSkHk @DuttonBooks\"]\n",
    " \n",
    "results = [clean_tweet(tw) for tw in tweets]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go and apply this to our own tweets\n",
    "my_results = [clean_tweet(tw) for tw in tweet_text_list]\n",
    "my_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b953b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules\n",
    "\n",
    "# Note that wordcloud package also provides a stop word list\n",
    "# We won't be using it here - but please do explore!!!\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comment_words = ''\n",
    "\n",
    "# iterate through list\n",
    "for tweet_text in results:\n",
    "    # split the text into tokens\n",
    "    tokens = tweet_text.split()\n",
    "    # Append tokens to string comment_words\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "# Let's make a word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                #stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
