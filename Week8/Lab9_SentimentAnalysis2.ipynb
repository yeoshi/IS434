{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Lab 9 - Sentiment Scoring (Part 2)\n",
    "\n",
    "In this script, we perform sentiment scoring using a rule-based approach.\n",
    "\n",
    "We have a pre-defined list of dictionaries (inside dict2/ directory):\n",
    "1) positive words\n",
    "2) negative words\n",
    "3) incrementers\n",
    "4) decrementers\n",
    "5) inverters\n",
    "\n",
    "Unlike in Part 1, sentiment words (positive/negative) are on a scale of -3 to 3.\n",
    "For example, some positive words are MORE positive than others. More positive words will have a more positive score.\n",
    "Likewise, some negative words are MORE negative than others. More negative words will have a more negative score.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Perform Part-Of-Speech (POS) tagging\n",
    "- Extract out Verbs, Nouns and Adjectives\n",
    "- Perform dictionary-based sentiment scoring\n",
    "--> If a term is identified to be positive, assign a score of +1/+2/+3 (least positive to most positive)\n",
    "--> If a term is identified to be negative, assign a score of -1/-2/-3 (least negative to most negative)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "dict_tagged_sentences = ''\n",
    "# Below indicates the relative path to\n",
    "# positive/negative/inverter/incrementer/decrementer files\n",
    "DICTIONARY_DIR_PREFIX = 'dicts2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        \"\"\"\n",
    "\n",
    "        :rtype : object\n",
    "        \"\"\"\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive1': return 1\n",
    "    if sentiment == 'positive2': return 2\n",
    "    if sentiment == 'positive3': return 3\n",
    "    if sentiment == 'negative1': return -1\n",
    "    if sentiment == 'negative2': return -2\n",
    "    if sentiment == 'negative3': return -3\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):\n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(sentences):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in sentences])\n",
    "\n",
    "\n",
    "def run_analysis(text):\n",
    "    splitter = Splitter() # This boy will split a long single string into sentences.\n",
    "    postagger = POSTagger() # This boy is the Part-Of-Speech tagger.\n",
    "\n",
    "    # If text contains multiple sentences, this line splits it into individual sentences.\n",
    "    splitted_sentences = splitter.split(text)\n",
    "    print (splitted_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    print (\"########## This performs Part-Of-Speech tagging. ##########\")\n",
    "    # This performs Part-Of-Speech tagging.\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint (pos_tagged_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    print (\"########## This line loads Positive word and Negative word corpus. ##########\")\n",
    "    # This line loads Positive word and Negative word dictionaries.\n",
    "    dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml'])\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    print (\"########## [Baseline Analysis] Using only Positive/Negative corpus. ##########\")\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    print (\"Score: %d\" % score)\n",
    "    #exit(1)\n",
    "\n",
    "    print (\"########## This line loads Positve/Negative corpus + incrementer/decrementer corpus. ##########\")\n",
    "    dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml'])\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    print (\"Score: %d\" % score)\n",
    "    #exit(1)\n",
    "\n",
    "    print (\"########## This line loads Positve/Negative corpus + incrementer/decrementer/inverter corpus. ##########\")\n",
    "    dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml', DICTIONARY_DIR_PREFIX + 'inv.yml'])\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    print (\"Score: %d\" % score)\n",
    "    #exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################### This is the MAIN section ###################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print (\"###############################\")\n",
    "\n",
    "    review_comment_text = \"This restaurant really sucks. The service was terrible. Food was horrible.\"\n",
    "    # Run sentiment scoring\n",
    "    run_analysis(review_comment_text)\n",
    "    \n",
    "    ###################### Challenge ######################\n",
    "    # Modify this script to read from 'Yelp_Food_Input.txt'\n",
    "    #   Each row is a unique \"review\" (textual)\n",
    "    # Open the TXT file and see how to extract the review content (which column contains the content?)\n",
    "    # Attempt to score each review\n",
    "    # Summarize the sentiment scores across ALL reviews using apprpriate charts, e.g. bar chart, box plot, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
