{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Crawling & Scraping (Google Search)\n",
    "\n",
    "This script performs a google search and returns URL that matches the search term provided.\n",
    "\n",
    "After collecting the urls, it visit each URL and extract text content (both HTML and PDF) from the pages visited.\n",
    "\n",
    "Author       : Eugene Choy (eugene.choy.wj@hotmail.com)\n",
    "Last Updated : 8 Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have already, you will need to nstall Python googlesearch package\n",
    "1. pip install google\n",
    "2. pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"wuhan\"\n",
    "my_results_list = []\n",
    "for i in search(query,        # The query you want to run\n",
    "                tld = 'com',  # The top level domain\n",
    "                lang = 'en',  # The language\n",
    "                num = 10,     # Number of results per page\n",
    "                start = 0,    # First result to retrieve\n",
    "                stop = 20,  # Last result to retrieve\n",
    "                pause = 2.0,  # Lapse between HTTP requests\n",
    "               ):\n",
    "    my_results_list.append(i)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the URLS into a text file\n",
    "\n",
    "import os \n",
    "\n",
    "\n",
    "# if directory don't exist\n",
    "path = \"./out/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Write into txt file for record\n",
    "time_string = str(int(time.time()))\n",
    "f = open(path + query.replace(\" \", \"_\") + time_string + \".txt\",\"w\")\n",
    "\n",
    "for result in my_results_list:\n",
    "    f.write(result +\"\\n\")\n",
    "    \n",
    "f.close()\n",
    "print(query.replace(\" \", \"_\") + time_string + \".txt\" + \" written to out folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import textract\n",
    "\n",
    "# Methods to extract content from webpage\n",
    "def get_urls_from_path(path,url_list):\n",
    "    for filename in os.listdir(path):\n",
    "        f = open(path + '/' + filename,'r')\n",
    "        for url in f:\n",
    "            url_list.append(url.replace(\"\\n\",\"\"))\n",
    "    return url_list\n",
    "\n",
    "\n",
    "def content_from_result(results_list):\n",
    "    content = \"\"\n",
    "    for result in results_list:\n",
    "        text = result.get_text()\n",
    "        text = text.replace(\"\\n\",\"\")\n",
    "        text = text.replace(\",\",\";\")\n",
    "        text = text.replace(\"  \",\"\")\n",
    "        text = text.replace(\"\\r\",\" \")\n",
    "        text = text.replace(\"\\xa0\",\" \")\n",
    "        text = text.replace(\"©\",\"copyrighted \")\n",
    "        content += text\n",
    "    return content\n",
    "\n",
    "def get_pdf_content_from_url(url):\n",
    "    myfile = requests.get(url)\n",
    "    \n",
    "    path = \"./temp/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    open(path + 'temp.pdf', 'wb').write(myfile.content)\n",
    "    text = textract.process(path + \"temp.pdf\")\n",
    "    \n",
    "    text = text.decode()\n",
    "    text = text.replace(\"\\n\",\"\")\n",
    "    text = text.replace(\"\\x0c\",\"\")\n",
    "    text = text.replace(\",\",\";\")\n",
    "    text = text.replace(\"  \",\"\")\n",
    "    text = text.replace(\"©\",\"copyrighted \")\n",
    "    \n",
    "    os.remove(path + \"temp.pdf\")\n",
    "    return text\n",
    "\n",
    "def get_content_from_url(url):\n",
    "\n",
    "    content = \"\"\n",
    "    if \".pdf\" not in url:\n",
    "        page = requests.get(url)\n",
    "        page_content = page.content\n",
    "\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "        results_list = soup.find_all('h2')\n",
    "        content += content_from_result(results_list)\n",
    "\n",
    "        results_list = soup.find_all('p')\n",
    "        content += \" \" + content_from_result(results_list)\n",
    "        return content\n",
    "    else:\n",
    "        return get_pdf_content_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "url_list = get_urls_from_path(\"./out\", url_list)\n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./csv/\"\n",
    "if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "csv = open(path + \"out.csv\",\"w\")\n",
    "\n",
    "csv.write(\"url,content\\n\")\n",
    "\n",
    "for url in url_list:\n",
    "    try:\n",
    "        csv.write(url + \",\" + get_content_from_url(url) + \"\\n\")\n",
    "        print(url + \" converted to text.\")\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(url + \" failed to convert to text.\")\n",
    "\n",
    "csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
