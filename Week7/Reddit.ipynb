{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Crawling & Scraping (Reddit)\n",
    "\n",
    "This script connect to Reddit API and crawls posts based on search terms.\n",
    "\n",
    "Ref: http://www.storybench.org/how-to-scrape-reddit-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting Reddit and subreddit instances\n",
    "\n",
    "PRAW stands for Python Reddit API Wrapper.\n",
    "\n",
    "First, we connect to Reddit by calling the praw.Reddit function and storing it in a variable.\n",
    "\n",
    "I’m calling mine reddit. You should pass the following arguments to that function:\n",
    "'''\n",
    "\n",
    "reddit = praw.Reddit(client_id='YOUR_CLIENT_ID', \\\n",
    "                     client_secret='YOUR_CLIENT_SECRET_KEY', \\\n",
    "                     user_agent='YOUR_APP_NAME', \\\n",
    "                     username='YOUR_REDDIT_USER_NAME', \\\n",
    "                     password='YOUR_REDDIT_LOGIN_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From that, we use the same logic to get to the subreddit we want and \n",
    "call the .subreddit instance from reddit and pass it the name of the subreddit we want to access.\n",
    "\n",
    "It can be found after “r/” in the subreddit’s URL.\n",
    "I’m going to use r/singapore, one of the subreddits we used in the story.\n",
    "Assign a new variable like this:\n",
    "'''\n",
    "subreddit = reddit.subreddit('singapore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Accessing the threads\n",
    "\n",
    "Each subreddit has five different ways of organizing the topics created by redditors:\n",
    ".hot, .new, .controversial, .top, .gilded\n",
    "\n",
    "You can also use .search(\"SEARCH_KEYWORDS\") to get only results matching an engine search.\n",
    "\n",
    "Let’s just grab the most up-voted topics all-time with the below.\n",
    "\n",
    "This will return a list-like object with the top-100 submission in r/singapore\n",
    "'''\n",
    "top_subreddit = subreddit.top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can control the size of the sample by passing a limit to .top(),\n",
    "but be aware that Reddit’s request limit* is 1000, like this:\n",
    "'''\n",
    "top_subreddit = subreddit.top(limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parsing and downloading the data\n",
    "\n",
    "We are right now really close to getting the data in our hands.\n",
    "Our top_subreddit object has methods to return all kinds of information from each submission.\n",
    "You can check it for yourself with these simple two lines:\n",
    "'''\n",
    "for submission in subreddit.top(limit=10):\n",
    "    print(submission.title, submission.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will scrape this information about the topics:\n",
    "  title, score, url, id, number of comments, date of creation, body text\n",
    "  \n",
    "This can be done very easily with a for lop just like above, but \n",
    "first we need to create a place to store the data.\n",
    "\n",
    "In Python, that is usually done with a dictionary. Let’s create it with the following code:\n",
    "'''\n",
    "topics_dict = { \"author\": [],\n",
    "                \"title\":[],\n",
    "                \"score\":[],\n",
    "                \"id\":[], \"url\":[],\n",
    "                \"comms_num\": [],\n",
    "                \"created\": [],\n",
    "                \"body\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we are ready to start scraping the data from the Reddit API.\n",
    "We will iterate through our top_subreddit object and append the information to our dictionary.\n",
    "'''\n",
    "for submission in top_subreddit:\n",
    "    topics_dict[\"author\"].append(submission.author)\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"id\"].append(submission.id)\n",
    "    topics_dict[\"url\"].append(submission.url)\n",
    "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "    topics_dict[\"created\"].append(submission.created)\n",
    "    topics_dict[\"body\"].append(submission.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Python dictionaries, however, are not very easy for us humans to read.\n",
    "This is where the Pandas module comes in handy.\n",
    "We’ll finally use it to put the data into something that\n",
    "looks like a spreadsheet — in Pandas, we call those Data Frames.\n",
    "'''\n",
    "topics_data = pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The data now looks like this:\n",
    "'''\n",
    "topics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fixing the date column\n",
    "\n",
    "Reddit uses UNIX timestamps to format date and time. \n",
    "Instead of manually converting all those entries, or using a site like \n",
    "www.unixtimestamp.com, we can easily write up a function in Python to automate that process.\n",
    "\n",
    "We define it, call it, and join the new column to dataset with the following code:\n",
    "'''\n",
    "\n",
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "_timestamp = topics_data[\"created\"].apply(get_date)\n",
    "\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The dataset now has a new column that we can understand and is ready to be exported.\n",
    "'''\n",
    "topics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exporting a CSV\n",
    "\n",
    "Pandas makes it very easy for us to create data files in various formats,\n",
    "including CSVs and Excel workbooks.\n",
    "\n",
    "To finish up the script, add the following to the end.\n",
    "'''\n",
    "\n",
    "topics_data.to_csv('Reddit_output.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retrieving a particular submission\n",
    "Ref: https://praw.readthedocs.io/en/latest/tutorials/comments.html\n",
    "\n",
    "Assume we want to process the comments for this submission:\n",
    "\n",
    "https://www.reddit.com/r/singapore/comments/sykq9h/humans_working_together_to_rescue_a_cat/\n",
    "\n",
    "\n",
    "First, we need to obtain a submission object. There are 2 ways to do this.\n",
    "1) Retrieve by URL\n",
    "2) Retrieve by submission ID (which we happen to know, it is 'sykq9h')\n",
    "'''\n",
    "\n",
    "submission = reddit.submission(url='https://www.reddit.com/r/singapore/comments/sykq9h/humans_working_together_to_rescue_a_cat/')\n",
    "#submission = reddit.submission(id='sykq9h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "With a submission object we can then interact with its CommentForest\n",
    "through the submission’s comments attribute. \n",
    "\n",
    "A CommentForest is a list of top-level comments each of which contains a CommentForest of replies.\n",
    "\n",
    "If we wanted to output only the body of the top level comments in the thread we could do:\n",
    "'''\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "While running this you will most likely encounter the exception\n",
    "\n",
    "AttributeError: 'MoreComments' object has no attribute 'body'\n",
    "\n",
    "This submission’s comment forest contains a number of MoreComments objects.\n",
    "\n",
    "These objects represent the “load more comments”, and “continue this thread” links \n",
    "encountered on the website.\n",
    "\n",
    "While we could ignore MoreComments in our code, like so:\n",
    "'''\n",
    "from praw.models import MoreComments\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A limit of None means that all MoreComments objects will be replaced until there are none left,\n",
    "as long as they satisfy the threshold.\n",
    "'''\n",
    "from praw.models import MoreComments\n",
    "\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we are able to successfully iterate over all the top-level comments.\n",
    "\n",
    "What about their replies? We could output all second-level comments like so:\n",
    "'''\n",
    "from praw.models import MoreComments\n",
    "\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "for top_level_comment in submission.comments:\n",
    "    if isinstance(top_level_comment, MoreComments):\n",
    "        continue\n",
    "    \n",
    "    print(\"=========== Top Level Comment ===========\")\n",
    "    print(top_level_comment.body)\n",
    "    \n",
    "    for second_level_comment in top_level_comment.replies:\n",
    "        print(\"      ############ Second Level Comment ############\")\n",
    "        print(second_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "However, the comment forest can be arbitrarily deep, so we’ll want a more robust solution.\n",
    "\n",
    "One way to iterate over a tree, or forest, is via a breadth-first traversal using a queue:\n",
    "'''\n",
    "submission.comments.replace_more(limit=None)\n",
    "comment_queue = submission.comments[:]  # Seed with top-level\n",
    "while comment_queue:\n",
    "    comment = comment_queue.pop(0)\n",
    "    print(comment.body)\n",
    "    comment_queue.extend(comment.replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The above code will output all the top-level comments, followed by second-level, third-level, etc. \n",
    "\n",
    "While it is awesome to be able to do your own breadth-first traversals, \n",
    "CommentForest provides a convenience method, list(), which returns a list of comments \n",
    "traversed in the same order as the code above.\n",
    "\n",
    "Thus the above can be rewritten as:\n",
    "'''\n",
    "\n",
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(\"=== Author: \", comment.author, \"===\")\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now properly extract and parse all (or most) of the comments belonging to a single submission.\n",
    "\n",
    "For more information about what attributes you can crawl:\n",
    "\n",
    "1) Submission\n",
    "https://praw.readthedocs.io/en/latest/code_overview/models/submission.html\n",
    "\n",
    "2) Comment\n",
    "https://praw.readthedocs.io/en/latest/code_overview/models/comment.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
