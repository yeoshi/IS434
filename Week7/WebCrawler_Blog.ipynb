{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Crawling & Scraping (Web Crawling - crawl ALL webpages of a blog)\n",
    "\n",
    "Web Crawling using Python\n",
    "\n",
    "We build a custom web crawler.\n",
    "\n",
    "It starts with the \"seed\" page (typically the first page or home page of a website).\n",
    "For example,\n",
    "    http://www.mongabong.com/\n",
    "The above is the \"seed\" page that we will use later on.\n",
    "\n",
    "This web crawler will:\n",
    "1. look for all anchor tags (or links) on index.html\n",
    "2. go into each of the above links and repeat 1)\n",
    "3. In the end, web crawler will have compiled a complete list of ALL webpages hosted at http://www.mongabong.com.\n",
    "\n",
    "'queue.txt' will be used to store all links extracted by web crawler.\n",
    "As each webpage in 'queue.txt' is searched - and its anchor tags identified, two events will happen:\n",
    "- The newly identified anchor tags' URLs (href) will be added into 'queue.txt'.\n",
    "- The once searched webpage from 'queue.txt' will be moved to 'crawled.txt'.\n",
    "'crawled.txt' contains all links our web crawler extracted AND these links were already searched for more anchor tags.\n",
    "\n",
    "In this lab, we will attempt to perform web crawling on:\n",
    "--> Mongchin Yeo's blog (local influencer): http://www.mongabong.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 1: Helper Functions #####\n",
    "'''\n",
    "The following are custom 'helper' functions that our web crawler will use later on.\n",
    "Largely, it deals with file writing, data appending, directory creation, etc.\n",
    "\n",
    "Run this code segment - to load the functions for later use.\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "# Each website is a separate project (folder)\n",
    "def create_project_dir(directory):\n",
    "    # If specified directory does NOT exist, then create one.\n",
    "    if not os.path.exists(directory):\n",
    "        print('Creating directory ' + directory)\n",
    "        os.makedirs(directory) # makedirs --> make directory\n",
    "\n",
    "\n",
    "# Create queue and crawled files (if not created)\n",
    "def create_data_files(project_name, base_url):\n",
    "    queue = os.path.join(project_name , 'queue.txt')\n",
    "    crawled = os.path.join(project_name, 'crawled.txt')\n",
    "    \n",
    "    if not os.path.isfile(queue):\n",
    "        # This part is VERY IMPORTANT\n",
    "        # \"queue.txt\" must have the SEED PAGE's URL\n",
    "        # Thus, we're going to write the SEED PAGE's URL (base_url) into this file.\n",
    "        write_file(queue, base_url)\n",
    "        \n",
    "    if not os.path.isfile(crawled):\n",
    "        write_file(crawled, '')\n",
    "\n",
    "\n",
    "# Create a new file\n",
    "def write_file(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "# Add data onto an existing file\n",
    "def append_to_file(path, data):\n",
    "    with open(path, 'a') as file:\n",
    "        file.write(data + '\\n')\n",
    "\n",
    "\n",
    "# Delete the contents of a file\n",
    "def delete_file_contents(path):\n",
    "    open(path, 'w').close()\n",
    "\n",
    "\n",
    "# Read a file and convert each line to set items\n",
    "def file_to_set(file_name):\n",
    "    results = set()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        for line in f:\n",
    "            results.add(line.replace('\\n', ''))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Iterate through a set, each item will be a line in a file\n",
    "def set_to_file(links, file_name):\n",
    "    with open(file_name,\"w\") as f:\n",
    "        for l in sorted(links):\n",
    "            f.write(l+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 2: LinkFinder class #####\n",
    "'''\n",
    "The below is a Class definition for LinkFinder.\n",
    "\n",
    "Given a webpage URL (page_url), LinkFinder will go into the webpage and\n",
    "looks for ALL <a> tags.\n",
    "\n",
    "For each <a> tags, e.g. <a href=\"http://www.mongabong.com/2019/09/maisha-from-closet-lover.html\">\n",
    "LinkFinder will extract out the link/URL: http://www.mongabong.com/2019/09/maisha-from-closet-lover.html\n",
    "\n",
    "page_links() method will return ALL links found in a given webpage (page_url).\n",
    "\n",
    "Run this code segment - to load the functions for later use.\n",
    "'''\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "class LinkFinder(HTMLParser):\n",
    "\n",
    "    # This constructor initializes base_url & page_url.\n",
    "    #    Example\n",
    "    #       base_url: http://www.mongabong.com\n",
    "    #       page_url: http://www.mongabong.com/2019/09/maisha-from-closet-lover.html\n",
    "    def __init__(self, base_url, page_url):\n",
    "        \n",
    "        #print('base_url: ' + base_url)\n",
    "        #print('page_url: ' + page_url)\n",
    "        #exit(1)\n",
    "        \n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.page_url = page_url\n",
    "        self.links = set()\n",
    "\n",
    "    # When we call HTMLParser feed(), this function is called when it encounters an opening tag <a>\n",
    "    # Given an <a> tag, e.g. <a href=\"http://www.mongabong.com/2019/09/maisha-from-closet-lover.html\">\n",
    "    #   extract attribute 'href' value --> http://www.mongabong.com/2019/09/maisha-from-closet-lover.html\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'a':\n",
    "            for (attribute, value) in attrs:\n",
    "                if attribute == 'href':\n",
    "                    url = parse.urljoin(self.base_url, value)\n",
    "                    self.links.add(url)\n",
    "\n",
    "    def page_links(self):\n",
    "        return self.links\n",
    "\n",
    "    def error(self, message):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 3: Domain Name Helper Functions #####\n",
    "'''\n",
    "The following are custom 'helper' functions that our web crawler will use later on.\n",
    "Largely, it deals with file writing, data appending, directory creation, etc.\n",
    "\n",
    "Run this code segment - to load the functions for later use.\n",
    "'''\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# Get domain name (example.com)\n",
    "# Given: http://www.mongabong.com/\n",
    "# returns the domain name without http://\n",
    "#    mongabong.com\n",
    "def get_domain_name(url):\n",
    "    try:\n",
    "        #print('===url: ' + url)\n",
    "        results = get_sub_domain_name(url).split('.')\n",
    "        #print(results[-2])\n",
    "        #print(results[-1])\n",
    "        #exit(1)\n",
    "        return results[-2] + '.' + results[-1]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "# Get sub domain name (name.example.com)\n",
    "# returns\n",
    "#       name\n",
    "def get_sub_domain_name(url):\n",
    "    try:\n",
    "        #print(urlparse(url).netloc)\n",
    "        #exit(1)\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 4: Spider class #####\n",
    "'''\n",
    "The below is a Class definition for Spider.\n",
    "\n",
    "This is the main class for Web Crawling.\n",
    "\n",
    "crawl_page is the main method.\n",
    "It takes a new web page URL (page_url) and initiates crawling.\n",
    "\n",
    "Run this code segment - to load the functions for later use.\n",
    "'''\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "class Spider:\n",
    "\n",
    "    project_name = ''\n",
    "    base_url = ''\n",
    "    domain_name = ''\n",
    "    queue_file = ''\n",
    "    crawled_file = ''\n",
    "    queue = set()\n",
    "    crawled = set()\n",
    "\n",
    "    def __init__(self, project_name, base_url, domain_name):\n",
    "        Spider.project_name = project_name\n",
    "        Spider.base_url = base_url\n",
    "        Spider.domain_name = domain_name\n",
    "        Spider.queue_file = Spider.project_name + '/queue.txt'\n",
    "        Spider.crawled_file = Spider.project_name + '/crawled.txt'\n",
    "        self.boot()\n",
    "        self.crawl_page('First spider', Spider.base_url)\n",
    "\n",
    "    # Creates directory and files for project on first run and starts the spider\n",
    "    @staticmethod\n",
    "    def boot():\n",
    "        create_project_dir(Spider.project_name)\n",
    "        create_data_files(Spider.project_name, Spider.base_url)\n",
    "        Spider.queue = file_to_set(Spider.queue_file)\n",
    "        Spider.crawled = file_to_set(Spider.crawled_file)\n",
    "\n",
    "    # Updates user display, fills queue and updates files\n",
    "    @staticmethod\n",
    "    def crawl_page(thread_name, page_url):\n",
    "        if page_url not in Spider.crawled:\n",
    "            print(thread_name + ' now crawling ' + page_url)\n",
    "            print('Queue ' + str(len(Spider.queue)) + ' | Crawled  ' + str(len(Spider.crawled)))\n",
    "            Spider.add_links_to_queue(Spider.gather_links(page_url))\n",
    "            Spider.queue.remove(page_url)\n",
    "            Spider.crawled.add(page_url)\n",
    "            Spider.update_files()\n",
    "\n",
    "    # Converts raw response data into readable information and checks for proper html formatting\n",
    "    @staticmethod\n",
    "    def gather_links(page_url):\n",
    "        html_string = ''\n",
    "        try:\n",
    "            response = urlopen(page_url)\n",
    "            if 'text/html' in response.getheader('Content-Type'):\n",
    "                html_bytes = response.read()\n",
    "                html_string = html_bytes.decode(\"utf-8\")\n",
    "            finder = LinkFinder(Spider.base_url, page_url)\n",
    "            #print(html_string) # prints HTML content of the current page\n",
    "            finder.feed(html_string)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set()\n",
    "        return finder.page_links()\n",
    "\n",
    "    # Saves queue data to project files\n",
    "    @staticmethod\n",
    "    def add_links_to_queue(links):\n",
    "        for url in links:\n",
    "            # If url is something that we already extracted and it's either\n",
    "            # - in queue to be crawled\n",
    "            # - in crawled.txt (already crawled)\n",
    "            # Then, ignore it - no need to process duplicates.\n",
    "            if (url in Spider.queue) or (url in Spider.crawled):\n",
    "                continue\n",
    "            \n",
    "            # VERY IMPORTANT\n",
    "            # In this lab, we will ONLY crawl... internal webpages.\n",
    "            # If an internal webpage points to http://cnn.com/entertainment/xyz.html\n",
    "            # We will IGNORE IT.\n",
    "            # How do we do it?\n",
    "            # We compare the domain name.\n",
    "            # For xyz.html, its domain is \"cnn.com\"\n",
    "            # Our domain name is supremeleader.today and it's NOT equal to \"cnn.com\"\n",
    "            # So we IGNORE\n",
    "            if Spider.domain_name != get_domain_name(url):\n",
    "                continue\n",
    "                \n",
    "            # ELSE\n",
    "            # We add this url to the queue\n",
    "            Spider.queue.add(url)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_files():\n",
    "        set_to_file(Spider.queue, Spider.queue_file)\n",
    "        set_to_file(Spider.crawled, Spider.crawled_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 5: MAIN #####\n",
    "'''\n",
    "This is where the user (you) initiates web crawling.\n",
    "\n",
    "PROJECT_NAME: this should uniquely identify the website you intend to crawl\n",
    "--> Later on, our code creates a directory (inside the current Jupyter directory).\n",
    "--> <current_directory>/<PROJECT_NAME>/queue.txt\n",
    "--> <current_directory>/<PROJECT_NAME>/crawled.txt\n",
    "\n",
    "NUMBER_OF_THREADS: specify desired number of threads\n",
    "--> Threading will create multiple instances of web crawler for \"parallel processing\"\n",
    "--> Increasing this to a very high number does not necessarily mean web crawling will be done faster\n",
    "-----> This depends on the computer server/machine on which web crawling is done.\n",
    "\n",
    "Run this code segment - to load the functions for later use.\n",
    "'''\n",
    "\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "PROJECT_NAME = 'mongabong'\n",
    "HOMEPAGE = 'http://www.mongabong.com/'\n",
    "\n",
    "DOMAIN_NAME = get_domain_name(HOMEPAGE)\n",
    "QUEUE_FILE = PROJECT_NAME + '/queue.txt'\n",
    "CRAWLED_FILE = PROJECT_NAME + '/crawled.txt'\n",
    "NUMBER_OF_THREADS = 2\n",
    "queue = Queue()\n",
    "Spider(PROJECT_NAME, HOMEPAGE, DOMAIN_NAME)\n",
    "\n",
    "\n",
    "# Create worker threads (will die when main exits)\n",
    "def create_workers():\n",
    "    for _ in range(NUMBER_OF_THREADS):\n",
    "        t = threading.Thread(target=work)\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "\n",
    "\n",
    "# Do the next job in the queue\n",
    "def work():\n",
    "    while True:\n",
    "        url = queue.get() # grab the next URL in the queue\n",
    "        Spider.crawl_page(threading.current_thread().name, url) # kick off web crawler\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "# Each queued link is a new job\n",
    "def create_jobs():\n",
    "    for link in file_to_set(QUEUE_FILE):\n",
    "        queue.put(link)\n",
    "    queue.join()\n",
    "    crawl()\n",
    "\n",
    "\n",
    "# Check if there are items in the queue, if so crawl them\n",
    "def crawl():\n",
    "    # 'queue.txt' should have the SEED PAGE's URL\n",
    "    # We start crawling from the SEED PAGE\n",
    "    queued_links = file_to_set(QUEUE_FILE)\n",
    "    if len(queued_links) > 0:\n",
    "        print(str(len(queued_links)) + ' links in the queue')\n",
    "        create_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "WARNING\n",
    "\n",
    "We're crawling someone else's website.\n",
    "For DEMO purposes, please run this segment for up to 10 seconds and STOP running this script.\n",
    "\n",
    "'''\n",
    "\n",
    "####### We're ready to kick off web crawler #######\n",
    "# Let's create worker(s)\n",
    "# If you specified NUMBER_OF_THREADS = 2,\n",
    "# this function will create 2 instances of Web Crawler\n",
    "# The two will parallel process web crawling.\n",
    "# This function does NOT start crawling yet - it will simply create and prep the workers.\n",
    "create_workers()\n",
    "\n",
    "# Let's crawl now\n",
    "crawl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
